From fc2b5f2edea3a35084a83fb32342e42430f290ac Mon Sep 17 00:00:00 2001
From: SeongJae Park <sj@kernel.org>
Date: Thu, 11 Apr 2024 16:08:52 -0700
Subject: [PATCH] ==== ACMA ====
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Subject: [RFC PATCH v0] mm/damon: introduce Access/Contiguity-aware Memory Auto-scaling (ACMA)

Extend DAMOS for getting temporal but exclusive owndership of contiguous
memory regions in access-aware way, and implement a module for
efficiently and automatically scaling system memory using the feature.

Motivation: Memory Overcommit Virtual Machine Systems
=====================================================

This work is motivated by an effort to improve efficiency and
reliability of a memory over-committed virtual machine system operation
method.  This section describes a business model using such systems, an
available approach, and its limitations.

Business Model
--------------

The business model services compute/memory resource for users'
workloads.  The service provider receives the workload from the user,
runs the workloads on their virtual guest machines, and returns the
workload's output back to the user.  The provider further calculates how
much of the resource is consumed by the workload, and ask the user to
pay the price for the real usage.

To maximize the host-level memory utilization while providing the
highest performance and lowest price to the user, the provider
automatically scales the guests' memory based on their estimation of the
workload's real memory demand, and overcommit the host's memory.  To
avoid low performance or high price resulting from the provider's
mistakes in the auto-scaling, the user can specify the minimum and
maximum amount of memory for their guest machine.

Existing Approach
-----------------

It is challenging to estimate real memory demand of guests from the host
in high accuracy.  Meanwhile, the service provider owns the control of
both the host and guests.  Therefore they use a guests-driven
cooperative management.  The guest reports unnecessary pages to the
host, and the host reallocates the reported pages to other guests.
Specifically, free pages reporting is being used.

The host-level reuse of the page is invisible to guests.  Guests can
simply use their pages regardless of the reporting.  If a guest access a
page that reported before, the host detects it from page fault, and give
the memory back to the guest.

Unless the guest is memory-frugal, only small amount of the guests'
memory is reported to the host, and the host-level memory utilization
drops.  To make the guests be memory-frugal with minimum performance
impact, the guests run access-aware proactive memory reclamation using
DAMON.  The basic structure of the system looks like below.

  ┌─────────────────────────────┐      ┌─────────┐
  │  Guest 1                    │      │ Guest 2 │
  │ ┌─────────────────────────┐ │      │         │
  │ │ DAMON-based ReClamation │ │      │         │
  │ └────────────┬────────────┘ │      │         │
  │              │ Reclaim      │      │         │
  │              ▼              │      │         │
  │ ┌─────────────────────────┐ │      │         │
  │ │  Free pages reporting   │ │      │         │
  │ └────────────┬────────────┘ │      │         │
  │              │              │      │         │
  └──────────────┼──────────────┘      └─────────┘
                 │ Report reclaimed         ▲
                 ▼ (free) pages             │ Alloc Guest 1
  ┌───────────────────────────────┐         │ freed memory
  │            Host               ├─────────┘
  └───────────────────────────────┘

The guest uses 4 KiB-size regular page by default while the host uses 2
MiB-size regular page for efficient management of the huge host-level
memory.  Hence, even if a guest reports a 4 KiB-page, the host cannot
use it unless its 511 neighbor pages are also reported.  Letting the
guest report every 4 KiB-page only increase the reporting overhead.
Hence the free pages reporting is tuned to work in 2 MiB granularity.
To avoid fragmented free pages not being reported, guests also
proactively run memory compaction ('/proc/sys/vm/compact_memory').

The provider further wants to minimize the 'struct page' overhead.  For
that, the guests continuously estimate real memory demands of the
running workload, and hot-[un]plug memory blocks with
'memory_hotplug.memmap_on_memory' so that 'struct page' objects for
offlined memory blocks can also be deallocated.  The guest kernel is
modified to let the user space do hot-[un]plug memory blocks, and report
the hot-unplugged memory block to the host.  This memory
hot-[un]plugging is also being used to keep the user-specified maximum
memory limit.

Limitations
-----------

Memory hot-unplugging is slow and easy to fail.  The problem mainly
comes from the fact that the operation requires isolating and migrating
pages in the block into other blocks, and the operation works in memory
block granularity, which is huge compared to pages.  The minimum amount
of works for doing it is not small, and the probability to meet
unmovable pages in the huge block is not low.  This makes the
guest-level memory scaling beocmes slow and unreliable, which results in
low host-level memory efficiency.

The system-level compaction is not optimized for only the reporting
purpose.  It could consume resource for compacting some part of memory
that anyway will not be able to be reported to the host for reuse.

Both hot-unplugging and compaction require pages isolations and
migrations, which are valid to fail for some reasons.  The operations
are better to be applied to cold pages first, since cold pages would
have lower probability to be pinned or making performance impact.  But
both hot-unplugging and compaction are access pattern oblivious.

There is no control to the reported pages.  This helps keeping the
system simple, but it makes reported pages reuse unreliable.  Any
reported page can be accessed again by the guest.  And even if only one
page among the reported 512 pages are accessed again, the entire 512
pages need to be returned to the guest.

The approach uses four kernel features, namely free pages reporting,
DAMON-based proactive reclamation, compaction, and memory
hot-[un]plugging.  Utilizing the four kernel features that not designed
to be used together for the specific case from user space in an
efficient way is somewhat challenging.

Design
======

We mitigate the limitations by introducing a way to preempt memory from
others of the system in access-aware way, and implementing a kernel
module that automatically scales the memory of the system in an
access/contiguity-aware way for the use case.

Access-aware Memory Preemption
------------------------------

DAMOS is for applying given memory operations in an access-aware manner.
Therefore, the major memory operations for improving the use case, page
isolation and migration with keeping the ownership in an acess-aware
way, can be implemented as DAMOS schemes.  For this, we design two new
DAMOS actions, namely 'preemption' and 'yield'.

'Preemption' DAMOS action isolates/migrates pages in the DAMOS target
memory region into pages out of the region, and get the ownership of the
pages of the region.  The action also receives minimum granularity to
apply the operation to.  In implementation, 'alloc_contig_range()' may
be used.  Once the operation is successfully applied, DAMOS does nothing
with the operation applied pages but notifies those to the scheme user.
Then the user can use the pages for thir purpose.  In other words,
'preemption' DAMOS action takes ownership of DAMON-found region of
specific access pattern in specific granularity and pass the ownership
to the user.  For example, the guest of the motivation use case can ask
DAMOS to 'preempt' cold pages and report those to the host.

'Yield' DAMOS action returns the ownership of the DAMOS target memory
region to the others of the system.  For safe ownership management, the
action notifies the scheme user the pages to apply 'yield' action so
that the user can safely forgive the ownership.  In the page fault based
memory overcommit use case, the user would need to do nothing.

Access/Contiguity-aware Memory Auto-scaling (ACMA)
--------------------------------------------------

Using the two new DAMOS actions, we design a kernel module for
substituting the abovely mentioned approach.  The module is called
Access/Contiguity-aware Memory Auto-scaling (ACMA).  ACMA receives three
user inputs.  Those are the minimum amount of memory to let the system
use ('min-mem'), the maximum amount of memory to let the system use
('max-mem'), and the acceptable level of memory pressure
('mem-pressure').  'Mem-pressure' is measured by memory pressure stall
information.  Then, it scales the memory of the system while meeting the
condition utilizing three DAMON-based operation schemes, namely
'reclaim', 'preempt', and 'yield'.

'Reclaim' scheme makes the system memory frugal and therefore easy to
migrate pages to some other place, aiming no-less-than 'mem-pressure'
memory pressure.  It reclaims memory of the system aiming the
'mem-pressure' amount of memory pressure, cold pages first.  In other
words, if the system's memory pressure level is lower than
'mem-pressure', it reclaims some coldest pages of the system.  The
amount of memory to reclaim is proportional to distance between current
pressure level and 'mem-pressure'.  If the memory pressure level becomes
higher than 'mem-pressure', it stops reclaiming memory until the memory
pressure level becomes lower than 'mem-pressure'.  This can be
implemented as a DAMOS scheme of 'pageout' action with a quota tuning
goal with 'mem-pressure' memory pressure target.

'Preempt' scheme scales the memory down aiming no-less-than
'mem-pressure' memory pressure.  It is implemented as a DAMOS scheme of
'preempt' action with 2 MiB operation granularity.  Similar to 'reclaim'
scheme, it has a quota tuning goal with 'mem-pressure' memory pressure
target.  For preempted 2 MiB granularity pages, it reports those to the
host so that the host can reuse.  The scheme has 'address range' type
scheme filter.  The filter makes the scheme to be applied to only the
not-completely-stolen memory block of highest starting memory address in
the physical address range starting from 'min-mem' and ending at the end
of the address space.  Once current filter-in-target address range is
entirely preempted, ACMA further hot-unplug the memory block to free up
'struct page' objects, and update the filter to apply the action to next
lower-address memory block.

'Yield' scheme scales the memory up aiming no-more-than 'mem-pressure'
memory pressure.  It is implemented as a DAMOS scheme of 'yield' action
with 2 MiB operation granularity.  It uses quota tuning goal with
'mem-pressure' memory pressure target like 'reclaim' and 'preempt', but
it notifies DAMOS that the aggressiveness of the scheme and the memory
pressure are inversely proportional.  Similar to 'preempt' scheme, it
uses an 'address range' type scheme filter.  The filter makes the scheme
to be applied to only the not-completely-stolen memory block of lowest
starting address in the physical address range starting from '0' and
ending at the 'max-mem' of the address space.  Once current 'yield'
target address range is entirely preempted, ACMA further hotplug the
next higher-address memory block and update the filter to apply the
action to the just-hotplugged memory block.

Below illustrates the different address ranges that the schemes are
applied.  Hence, the effective memory size of the system starts from
'end' bytes, and automatically be changed depends on the memory pressure
of the system.  Once it becomes lower than 'max' bytes, it will never
get greater than 'max' bytes.  It can also never gets lower than 'min'
bytes.  Because all the schemes auto-tune their aggressiveness based on
'mem-pressure', the system's memory pressure cannot exceed the
user-acceptable level.

    Proactive Reclaim
             │
  ┌──────────┴────────────┐
  │                       │
  │            Preempt    │
                 │
         ┌───────┴────────┐
         │                │
  0     min     max     end (memory address)
  │              │
  └──────┬───────┘
         │
       Yield

  < address ranges that the schemes are applied >

Possible Future Usage of Access-aware Memory Preemption
=======================================================

Contiguous Memory Allocation
----------------------------

For contiguous memory allocation, a large contiguous memory pool is
required.  Current approaches reserve such regions in early boot time
before the memory is fragmented, or define specific parts of the memory
as special type of zones utilizing contiguous memory allocation friendly
policies.  Reservation-based approach can waste memory, while zone-based
approach has optimum zone size finding challenge.

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 damon_meta_changes/7Q37ed37 | 0
 1 file changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 damon_meta_changes/7Q37ed37

diff --git a/damon_meta_changes/7Q37ed37 b/damon_meta_changes/7Q37ed37
new file mode 100644
index 000000000000..e69de29bb2d1
-- 
2.39.2

