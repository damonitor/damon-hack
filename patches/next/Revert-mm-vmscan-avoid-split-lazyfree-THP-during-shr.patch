From c9a1afe444167fd6f46d5b8b515270c274e72a34 Mon Sep 17 00:00:00 2001
From: SeongJae Park <sj@kernel.org>
Date: Mon, 29 Apr 2024 13:22:14 -0700
Subject: [PATCH] Revert "mm/vmscan: avoid split lazyfree THP during
 shrink_folio_list()"

This reverts commit 6ae76ae5b2e07d4eb6b3e4fc594856d6500ade54.

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 include/linux/huge_mm.h |  2 --
 mm/huge_memory.c        | 75 -----------------------------------------
 mm/rmap.c               |  3 --
 3 files changed, 80 deletions(-)

diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index fd330f72b4f3..2daadfcc6776 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -38,8 +38,6 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		    unsigned long cp_flags);
 void split_huge_pmd_locked(struct vm_area_struct *vma, unsigned long address,
 			   pmd_t *pmd, bool freeze, struct folio *folio);
-bool unmap_huge_pmd_locked(struct vm_area_struct *vma, unsigned long addr,
-			   pmd_t *pmdp, struct folio *folio);
 
 vm_fault_t vmf_insert_pfn_pmd(struct vm_fault *vmf, pfn_t pfn, bool write);
 vm_fault_t vmf_insert_pfn_pud(struct vm_fault *vmf, pfn_t pfn, bool write);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index d35d526ed48f..145505a1dd05 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2690,81 +2690,6 @@ static void unmap_folio(struct folio *folio)
 	try_to_unmap_flush();
 }
 
-static bool __discard_trans_pmd_locked(struct vm_area_struct *vma,
-				       unsigned long addr, pmd_t *pmdp,
-				       struct folio *folio)
-{
-	struct mm_struct *mm = vma->vm_mm;
-	int ref_count, map_count;
-	pmd_t orig_pmd = *pmdp;
-	struct mmu_gather tlb;
-	struct page *page;
-
-	if (pmd_dirty(orig_pmd) || folio_test_dirty(folio))
-		return false;
-	if (unlikely(!pmd_present(orig_pmd) || !pmd_trans_huge(orig_pmd)))
-		return false;
-
-	page = pmd_page(orig_pmd);
-	if (unlikely(page_folio(page) != folio))
-		return false;
-
-	tlb_gather_mmu(&tlb, mm);
-	orig_pmd = pmdp_huge_get_and_clear(mm, addr, pmdp);
-	tlb_remove_pmd_tlb_entry(&tlb, pmdp, addr);
-
-	/*
-	 * Syncing against concurrent GUP-fast:
-	 * - clear PMD; barrier; read refcount
-	 * - inc refcount; barrier; read PMD
-	 */
-	smp_mb();
-
-	ref_count = folio_ref_count(folio);
-	map_count = folio_mapcount(folio);
-
-	/*
-	 * Order reads for folio refcount and dirty flag
-	 * (see comments in __remove_mapping()).
-	 */
-	smp_rmb();
-
-	/*
-	 * If the PMD or folio is redirtied at this point, or if there are
-	 * unexpected references, we will give up to discard this folio
-	 * and remap it.
-	 *
-	 * The only folio refs must be one from isolation plus the rmap(s).
-	 */
-	if (ref_count != map_count + 1 || folio_test_dirty(folio) ||
-	    pmd_dirty(orig_pmd)) {
-		set_pmd_at(mm, addr, pmdp, orig_pmd);
-		return false;
-	}
-
-	folio_remove_rmap_pmd(folio, page, vma);
-	zap_deposited_table(mm, pmdp);
-	add_mm_counter(mm, MM_ANONPAGES, -HPAGE_PMD_NR);
-	folio_put(folio);
-
-	return true;
-}
-
-bool unmap_huge_pmd_locked(struct vm_area_struct *vma, unsigned long addr,
-			   pmd_t *pmdp, struct folio *folio)
-{
-	VM_WARN_ON_FOLIO(!folio_test_pmd_mappable(folio), folio);
-	VM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);
-	VM_WARN_ON_ONCE(!IS_ALIGNED(addr, HPAGE_PMD_SIZE));
-
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
-	if (folio_test_anon(folio) && !folio_test_swapbacked(folio))
-		return __discard_trans_pmd_locked(vma, addr, pmdp, folio);
-#endif
-
-	return false;
-}
-
 static void remap_page(struct folio *folio, unsigned long nr)
 {
 	int i = 0;
diff --git a/mm/rmap.c b/mm/rmap.c
index 087a79f1f611..43e44b815fda 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1677,9 +1677,6 @@ static bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,
 		}
 
 		if (!pvmw.pte && (flags & TTU_SPLIT_HUGE_PMD)) {
-			if (unmap_huge_pmd_locked(vma, range.start, pvmw.pmd,
-						  folio))
-				goto walk_done;
 			/*
 			 * We temporarily have to drop the PTL and start once
 			 * again from that now-PTE-mapped page table.
-- 
2.39.2

