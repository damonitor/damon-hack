From: SeongJae Park <sj@kernel.org>
Date: Wed, 5 Feb 2025 14:30:06 -0800
Subject: [PATCH] mm/damon: avoid double-applying DAMOS action to same large
 folios

Reported-by: Usama Arif <usamaarif642@gmail.com>
Closes: https://lore.kernel.org/20250203225604.44742-3-usamaarif642@gmail.com
Signed-off-by: SeongJae Park <sj@kernel.org>
---
 include/linux/damon.h | 10 +++++++++-
 mm/damon/core.c       | 26 +++++++++++++++++++++++++-
 mm/damon/paddr.c      | 43 +++++++++++++++++++++++++++++--------------
 mm/damon/vaddr.c      |  3 ++-
 4 files changed, 65 insertions(+), 17 deletions(-)

diff --git a/include/linux/damon.h b/include/linux/damon.h
index 9b6f39c4873e..776665968936 100644
--- a/include/linux/damon.h
+++ b/include/linux/damon.h
@@ -506,6 +506,8 @@ struct damos {
 	unsigned long next_apply_sis;
 	/* informs if ongoing DAMOS walk for this scheme is finished */
 	bool walk_completed;
+	/* bytes to skip applying action on next regions */
+	unsigned long sz_to_skip;
 /* public: */
 	struct damos_quota quota;
 	struct damos_watermarks wmarks;
@@ -582,6 +584,11 @@ enum damon_ops_id {
  * to the region and return bytes of the region that the action is successfully
  * applied.  It should also report how many bytes of the region has passed
  * filters (&struct damos_filter) that handled by itself.
+ * If the action has to be applied to memory beyond the end of the region
+ * (e.g., the action can be applied in hugepage granularity only but a part of
+ * the hugepage belongs to next DAMON regions), @sz_over_applied should be set
+ * for the amount, so that kdamond can skip asking applying the action to the
+ * memory twice or even more.
  * @target_valid should check whether the target is still valid for the
  * monitoring.
  * @cleanup is called from @kdamond just before its termination.
@@ -598,7 +605,8 @@ struct damon_operations {
 			struct damos *scheme);
 	unsigned long (*apply_scheme)(struct damon_ctx *context,
 			struct damon_target *t, struct damon_region *r,
-			struct damos *scheme, unsigned long *sz_filter_passed);
+			struct damos *scheme, unsigned long *sz_filter_passed,
+			unsigned long *sz_over_applied);
 	bool (*target_valid)(struct damon_target *t);
 	void (*cleanup)(struct damon_ctx *context);
 };
diff --git a/mm/damon/core.c b/mm/damon/core.c
index fa736318c8b0..113cd6b35f83 100644
--- a/mm/damon/core.c
+++ b/mm/damon/core.c
@@ -1633,6 +1633,25 @@ static void damos_walk_cancel(struct damon_ctx *ctx)
 	mutex_unlock(&ctx->walk_control_lock);
 }
 
+static bool damos_should_skip(struct damon_target *t, struct damon_region *r,
+		struct damos *s)
+{
+	unsigned long sz = damon_sz_region(r);
+
+	if (!s->sz_to_skip)
+		return false;
+	if (s->sz_to_skip < sz) {
+		damon_split_region_at(t, r, s->sz_to_skip);
+		s->sz_to_skip = 0;
+		return true;
+	}
+	if (r == damon_last_region(t))
+		s->sz_to_skip = 0;
+	else
+		s->sz_to_skip -= sz;
+	return true;
+}
+
 static void damos_apply_scheme(struct damon_ctx *c, struct damon_target *t,
 		struct damon_region *r, struct damos *s)
 {
@@ -1641,6 +1660,7 @@ static void damos_apply_scheme(struct damon_ctx *c, struct damon_target *t,
 	struct timespec64 begin, end;
 	unsigned long sz_applied = 0;
 	unsigned long sz_ops_filter_passed = 0;
+	unsigned long sz_over_applied = 0;
 	int err = 0;
 	/*
 	 * We plan to support multiple context per kdamond, as DAMON sysfs
@@ -1687,6 +1707,8 @@ static void damos_apply_scheme(struct damon_ctx *c, struct damon_target *t,
 			}
 			damon_split_region_at(t, r, sz);
 		}
+		if (damos_should_skip(t, r, s))
+			return;
 		if (damos_filter_out(c, t, r, s))
 			return;
 		ktime_get_coarse_ts64(&begin);
@@ -1696,9 +1718,11 @@ static void damos_apply_scheme(struct damon_ctx *c, struct damon_target *t,
 			trace_damos_before_apply(cidx, sidx, tidx, r,
 					damon_nr_regions(t), do_trace);
 			sz_applied = c->ops.apply_scheme(c, t, r, s,
-					&sz_ops_filter_passed);
+					&sz_ops_filter_passed,
+					&sz_over_applied);
 		}
 		damos_walk_call_walk(c, t, r, s, sz_ops_filter_passed);
+		s->sz_to_skip = sz_over_applied;
 		ktime_get_coarse_ts64(&end);
 		quota->total_charged_ns += timespec64_to_ns(&end) -
 			timespec64_to_ns(&begin);
diff --git a/mm/damon/paddr.c b/mm/damon/paddr.c
index 7f39d96789a3..9e94151062a6 100644
--- a/mm/damon/paddr.c
+++ b/mm/damon/paddr.c
@@ -392,7 +392,8 @@ static bool damos_pa_filter_out(struct damos *scheme, struct folio *folio)
 }
 
 static unsigned long damon_pa_pageout(struct damon_region *r, struct damos *s,
-		unsigned long *sz_filter_passed)
+		unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	unsigned long addr, applied;
 	LIST_HEAD(folio_list);
@@ -444,12 +445,14 @@ static unsigned long damon_pa_pageout(struct damon_region *r, struct damos *s,
 		damos_destroy_filter(filter);
 	applied = reclaim_pages(&folio_list);
 	cond_resched();
+	*sz_over_applied = addr - r->ar.end;
 	return applied * PAGE_SIZE;
 }
 
 static inline unsigned long damon_pa_mark_accessed_or_deactivate(
 		struct damon_region *r, struct damos *s, bool mark_accessed,
-		unsigned long *sz_filter_passed)
+		unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	unsigned long addr, applied = 0;
 
@@ -476,21 +479,24 @@ static inline unsigned long damon_pa_mark_accessed_or_deactivate(
 		addr += folio_size(folio);
 		folio_put(folio);
 	}
+	*sz_over_applied = addr - r->ar.end;
 	return applied * PAGE_SIZE;
 }
 
 static unsigned long damon_pa_mark_accessed(struct damon_region *r,
-	struct damos *s, unsigned long *sz_filter_passed)
+	struct damos *s, unsigned long *sz_filter_passed,
+	unsigned long *sz_over_applied)
 {
 	return damon_pa_mark_accessed_or_deactivate(r, s, true,
-			sz_filter_passed);
+			sz_filter_passed, sz_over_applied);
 }
 
 static unsigned long damon_pa_deactivate_pages(struct damon_region *r,
-	struct damos *s, unsigned long *sz_filter_passed)
+	struct damos *s, unsigned long *sz_filter_passed,
+	unsigned long *sz_over_applied)
 {
 	return damon_pa_mark_accessed_or_deactivate(r, s, false,
-			sz_filter_passed);
+			sz_filter_passed, sz_over_applied);
 }
 
 static unsigned int __damon_pa_migrate_folio_list(
@@ -615,7 +621,8 @@ static unsigned long damon_pa_migrate_pages(struct list_head *folio_list,
 }
 
 static unsigned long damon_pa_migrate(struct damon_region *r, struct damos *s,
-		unsigned long *sz_filter_passed)
+		unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	unsigned long addr, applied;
 	LIST_HEAD(folio_list);
@@ -643,6 +650,7 @@ static unsigned long damon_pa_migrate(struct damon_region *r, struct damos *s,
 	}
 	applied = damon_pa_migrate_pages(&folio_list, s->target_nid);
 	cond_resched();
+	*sz_over_applied = addr - r->ar.end;
 	return applied * PAGE_SIZE;
 }
 
@@ -656,7 +664,8 @@ static bool damon_pa_scheme_has_filter(struct damos *s)
 }
 
 static unsigned long damon_pa_stat(struct damon_region *r, struct damos *s,
-		unsigned long *sz_filter_passed)
+		unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	unsigned long addr;
 	LIST_HEAD(folio_list);
@@ -678,6 +687,7 @@ static unsigned long damon_pa_stat(struct damon_region *r, struct damos *s,
 		addr += folio_size(folio);
 		folio_put(folio);
 	}
+	*sz_over_applied = addr - r->ar.end;
 	return 0;
 }
 
@@ -770,18 +780,23 @@ static unsigned long damon_pa_alloc_or_free(
 
 static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
 		struct damon_target *t, struct damon_region *r,
-		struct damos *scheme, unsigned long *sz_filter_passed)
+		struct damos *scheme, unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	switch (scheme->action) {
 	case DAMOS_PAGEOUT:
-		return damon_pa_pageout(r, scheme, sz_filter_passed);
+		return damon_pa_pageout(r, scheme, sz_filter_passed,
+				sz_over_applied);
 	case DAMOS_LRU_PRIO:
-		return damon_pa_mark_accessed(r, scheme, sz_filter_passed);
+		return damon_pa_mark_accessed(r, scheme, sz_filter_passed,
+				sz_over_applied);
 	case DAMOS_LRU_DEPRIO:
-		return damon_pa_deactivate_pages(r, scheme, sz_filter_passed);
+		return damon_pa_deactivate_pages(r, scheme, sz_filter_passed,
+				sz_over_applied);
 	case DAMOS_MIGRATE_HOT:
 	case DAMOS_MIGRATE_COLD:
-		return damon_pa_migrate(r, scheme, sz_filter_passed);
+		return damon_pa_migrate(r, scheme, sz_filter_passed,
+				sz_over_applied);
 #ifdef CONFIG_ACMA
 	case DAMOS_ALLOC:
 		return damon_pa_alloc_or_free(r, scheme, true);
@@ -789,7 +804,7 @@ static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
 		return damon_pa_alloc_or_free(r, scheme, false);
 #endif
 	case DAMOS_STAT:
-		return damon_pa_stat(r, scheme, sz_filter_passed);
+		return damon_pa_stat(r, scheme, sz_filter_passed, sz_over_applied);
 	default:
 		/* DAMOS actions that not yet supported by 'paddr'. */
 		break;
diff --git a/mm/damon/vaddr.c b/mm/damon/vaddr.c
index a6174f725bd7..16c0db8ea1e0 100644
--- a/mm/damon/vaddr.c
+++ b/mm/damon/vaddr.c
@@ -655,7 +655,8 @@ static unsigned long damos_madvise(struct damon_target *target,
 
 static unsigned long damon_va_apply_scheme(struct damon_ctx *ctx,
 		struct damon_target *t, struct damon_region *r,
-		struct damos *scheme, unsigned long *sz_filter_passed)
+		struct damos *scheme, unsigned long *sz_filter_passed,
+		unsigned long *sz_over_applied)
 {
 	int madv_action;
 
-- 
2.39.5

