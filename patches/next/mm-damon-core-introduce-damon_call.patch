From: SeongJae Park <sj@kernel.org>
Date: Fri, 15 Nov 2024 12:07:18 -0800
Subject: [PATCH] mm/damon/core: introduce damon_call()

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 include/linux/damon.h | 21 +++++++++++++++
 mm/damon/core.c       | 60 +++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 81 insertions(+)

diff --git a/include/linux/damon.h b/include/linux/damon.h
index 0d56a9b272df..7c2668a4bb39 100644
--- a/include/linux/damon.h
+++ b/include/linux/damon.h
@@ -577,6 +577,22 @@ struct damon_callback {
 	void (*before_terminate)(struct damon_ctx *context);
 };
 
+/*
+ * struct damon_call_req - Request for damon_call().
+ *
+ * @fn:			Function to be called back from DAMON.
+ * @arg:		Argument that will be passed to @fn.
+ * @return_code:	Return code from @fn.
+ */
+struct damon_call_req {
+	int (*fn)(void *arg);
+	void *arg;
+	int return_code;
+/* private: internal use only */
+	/* for waiting on DAMON's @fn invocation completion. */
+	struct completion completion;
+};
+
 /**
  * struct damon_attrs - Monitoring attributes for accuracy/overhead control.
  *
@@ -657,6 +673,9 @@ struct damon_ctx {
 	/* for scheme quotas prioritization */
 	unsigned long *regions_score_histogram;
 
+	struct damon_call_req *call_request;
+	struct mutex call_request_lock;
+
 /* public: */
 	struct task_struct *kdamond;
 	struct mutex kdamond_lock;
@@ -804,6 +823,8 @@ static inline unsigned int damon_max_nr_accesses(const struct damon_attrs *attrs
 int damon_start(struct damon_ctx **ctxs, int nr_ctxs, bool exclusive);
 int damon_stop(struct damon_ctx **ctxs, int nr_ctxs);
 
+int damon_call(struct damon_ctx *ctx, struct damon_call_req *req);
+
 int damon_set_region_biggest_system_ram_default(struct damon_target *t,
 				unsigned long *start, unsigned long *end);
 
diff --git a/mm/damon/core.c b/mm/damon/core.c
index fb11c1d42221..3930a3ea6929 100644
--- a/mm/damon/core.c
+++ b/mm/damon/core.c
@@ -533,6 +533,7 @@ struct damon_ctx *damon_new_ctx(void)
 	ctx->next_ops_update_sis = 0;
 
 	mutex_init(&ctx->kdamond_lock);
+	mutex_init(&ctx->call_request_lock);
 
 	ctx->attrs.min_nr_regions = 10;
 	ctx->attrs.max_nr_regions = 1000;
@@ -1183,6 +1184,39 @@ int damon_stop(struct damon_ctx **ctxs, int nr_ctxs)
 	return err;
 }
 
+/**
+ * damon_call() - Invoke a given function on a DAMON worker thread.
+ * @ctx:	The DAMON context to execute the request on.
+ * @req:	The call request specification.
+ *
+ * Ask DAMON worker thread of @ctx to call a given function as specified in
+ * @req and wait for the result.  DAMON worker thread will invoke the requested
+ * function only once, on the fastest one of following events.  After sampling,
+ * after aggregation, after watermarks check, and before termination of the
+ * DAMON context.  The return code of the callback function will be saved in
+ * &->return_code of @req.  The caller of this function will wait until the
+ * invocation is completed.
+ *
+ * If a parallel damon_call() for same @ctx exists, only one wins and others
+ * return -EBUSY.
+ *
+ * Return: 0 on success, negative error code otherwise.
+ */
+int damon_call(struct damon_ctx *ctx, struct damon_call_req *req)
+{
+	init_completion(&req->completion);
+	mutex_lock(&ctx->call_request_lock);
+	if (ctx->call_request) {
+		mutex_unlock(&ctx->call_request_lock);
+		return -EBUSY;
+	}
+	ctx->call_request = req;
+	mutex_unlock(&ctx->call_request_lock);
+
+	wait_for_completion(&req->completion);
+	return 0;
+}
+
 /*
  * Reset the aggregated monitoring results ('nr_accesses' of each region).
  */
@@ -1974,6 +2008,25 @@ static void kdamond_usleep(unsigned long usecs)
 		usleep_idle_range(usecs, usecs + 1);
 }
 
+static int kdamond_callback(struct damon_ctx *ctx)
+{
+	struct damon_call_req *req;
+	int ret;
+
+	mutex_lock(&ctx->call_request_lock);
+	req = ctx->call_request;
+	if (!req) {
+		mutex_unlock(&ctx->call_request_lock);
+		return 0;
+	}
+	ret = req->fn(req->arg);
+	req->return_code = ret;
+	complete(&req->completion);
+	ctx->call_request = NULL;
+	mutex_unlock(&ctx->call_request_lock);
+	return ret;
+}
+
 /* Returns negative error code if it's not activated but should return */
 static int kdamond_wait_activation(struct damon_ctx *ctx)
 {
@@ -1998,6 +2051,8 @@ static int kdamond_wait_activation(struct damon_ctx *ctx)
 		if (ctx->callback.after_wmarks_check &&
 				ctx->callback.after_wmarks_check(ctx))
 			break;
+		if (kdamond_callback(ctx))
+			break;
 	}
 	return -EBUSY;
 }
@@ -2067,6 +2122,8 @@ static int kdamond_fn(void *data)
 		if (ctx->callback.after_sampling &&
 				ctx->callback.after_sampling(ctx))
 			break;
+		if (kdamond_callback(ctx))
+			break;
 
 		kdamond_usleep(sample_interval);
 		ctx->passed_sample_intervals++;
@@ -2081,6 +2138,8 @@ static int kdamond_fn(void *data)
 			if (ctx->callback.after_aggregation &&
 					ctx->callback.after_aggregation(ctx))
 				break;
+			if (kdamond_callback(ctx))
+				break;
 		}
 
 		/*
@@ -2119,6 +2178,7 @@ static int kdamond_fn(void *data)
 
 	if (ctx->callback.before_terminate)
 		ctx->callback.before_terminate(ctx);
+	kdamond_callback(ctx);
 	if (ctx->ops.cleanup)
 		ctx->ops.cleanup(ctx);
 	kfree(ctx->regions_score_histogram);
-- 
2.39.5

