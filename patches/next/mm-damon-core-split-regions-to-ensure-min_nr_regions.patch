From: SeongJae Park <sj@kernel.org>
Date: Sat, 14 Feb 2026 11:31:49 -0800
Subject: [PATCH] mm/damon/core: split regions to ensure min_nr_regions

DAMON repsect min_nr_regions parameter by setting maximum size of
each region as total monitoring region size divided by the parameter
value.  And the limit is applied by preventing merge of regions that
result in a region larger than the maximum size.  There is no such
mechanism in split side.  If the current number of regions is smaller
than the min_nr_regions, therefore, it takes time until the number of
regions become equal to or larger than the min_nr_regions.

This makes fixed granularity monitoring difficult, especially when the
aggregation interval is long.  There was actually a report [1] of the
case.

Ensure min_nr_regions is respected, by splitting regions larger than the
size.  Note that the size limit is aligned by damon_ctx->min_region_sz
and cannot be zero.  That is, if min_nr_region is larger than the entire
monitoring regions size divided by ->min_region_sz, that cannot be
respected.

[1] https://lore.kernel.org/CAC5umyjmJE9SBqjbetZZecpY54bHpn2AvCGNv3aF6J=1cfoPXQ@mail.gmail.com

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 mm/damon/core.c | 30 ++++++++++++++++++++++++------
 1 file changed, 24 insertions(+), 6 deletions(-)

diff --git a/mm/damon/core.c b/mm/damon/core.c
index 1cfa08ad641ad..9582a9e21bc77 100644
--- a/mm/damon/core.c
+++ b/mm/damon/core.c
@@ -1307,12 +1307,33 @@ static unsigned long damon_region_sz_limit(struct damon_ctx *ctx)
 
 	if (ctx->attrs.min_nr_regions)
 		sz /= ctx->attrs.min_nr_regions;
-	if (sz < ctx->min_region_sz)
+	if (!sz)
 		sz = ctx->min_region_sz;
+	sz = ALIGN(sz, ctx->min_region_sz);
 
 	return sz;
 }
 
+static void damon_split_region_at(struct damon_target *t,
+				  struct damon_region *r, unsigned long sz_r);
+
+static unsigned long kdamond_get_apply_max_region_sz(struct damon_ctx *ctx)
+{
+	unsigned long max_region_sz = damon_region_sz_limit(ctx);
+	struct damon_target *t;
+	struct damon_region *r, *next;
+
+	damon_for_each_target(t, ctx) {
+		damon_for_each_region_safe(r, next, t) {
+			while (damon_sz_region(r) > max_region_sz) {
+				damon_split_region_at(t, r, max_region_sz);
+				r = damon_next_region(r);
+			}
+		}
+	}
+	return max_region_sz;
+}
+
 static int kdamond_fn(void *data);
 
 /*
@@ -1664,9 +1685,6 @@ static void kdamond_tune_intervals(struct damon_ctx *c)
 	damon_set_attrs(c, &new_attrs);
 }
 
-static void damon_split_region_at(struct damon_target *t,
-				  struct damon_region *r, unsigned long sz_r);
-
 static bool __damos_valid_target(struct damon_region *r, struct damos *s)
 {
 	unsigned long sz;
@@ -2770,7 +2788,7 @@ static int kdamond_fn(void *data)
 	if (!ctx->regions_score_histogram)
 		goto done;
 
-	sz_limit = damon_region_sz_limit(ctx);
+	sz_limit = kdamond_get_apply_max_region_sz(ctx);
 
 	while (!kdamond_need_stop(ctx)) {
 		/*
@@ -2855,7 +2873,7 @@ static int kdamond_fn(void *data)
 				sample_interval;
 			if (ctx->ops.update)
 				ctx->ops.update(ctx);
-			sz_limit = damon_region_sz_limit(ctx);
+			sz_limit = kdamond_get_apply_max_region_sz(ctx);
 		}
 	}
 done:
-- 
2.47.3

