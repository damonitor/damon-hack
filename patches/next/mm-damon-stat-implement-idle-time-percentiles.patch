From: SeongJae Park <sj@kernel.org>
Date: Sun, 11 May 2025 13:26:59 -0700
Subject: [PATCH] mm/damon/stat: implement idle time percentiles

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 mm/damon/stat.c | 71 +++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 71 insertions(+)

diff --git a/mm/damon/stat.c b/mm/damon/stat.c
index 2814a893d96b..8f61faf6fe42 100644
--- a/mm/damon/stat.c
+++ b/mm/damon/stat.c
@@ -9,6 +9,7 @@
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
+#include <linux/sort.h>
 
 #ifdef MODULE_PARAM_PREFIX
 #undef MODULE_PARAM_PREFIX
@@ -30,8 +31,77 @@ MODULE_PARM_DESC(enable, "Enable of disable DAMON_STAT");
 static unsigned long estimated_memory_bandwidth __read_mostly = 0;
 module_param(estimated_memory_bandwidth, ulong, 0400);
 
+static unsigned long percentiles_ms[101] __read_mostly = {0,};
+module_param_array(percentiles_ms, ulong, NULL, 0400);
+
 static struct damon_ctx *damon_stat_context;
 
+static unsigned int damon_stat_idletime(const struct damon_region *r)
+{
+	if (r->nr_accesses)
+		return 0;
+	return r->age + 1;
+}
+
+static int damon_stat_cmp_regions(const void *a, const void *b)
+{
+	const struct damon_region **ra = a;
+	const struct damon_region **rb = b;
+
+	return damon_stat_idletime(*ra) - damon_stat_idletime(*rb);
+}
+
+static int damon_stat_sort_regions(struct damon_ctx *c,
+		struct damon_region ***sorted_ptr, int *nr_regions_ptr,
+		unsigned long *total_sz_ptr)
+{
+	struct damon_target *t;
+	struct damon_region *r;
+	struct damon_region **region_pointers;
+	unsigned int nr_regions = 0;
+	unsigned long total_sz = 0;
+
+	damon_for_each_target(t, c) {
+		/* there is only one target */
+		region_pointers = kmalloc_array(damon_nr_regions(t),
+				sizeof(*region_pointers), GFP_KERNEL);
+		if (!region_pointers)
+			return -ENOMEM;
+		damon_for_each_region(r, t) {
+			region_pointers[nr_regions++] = r;
+			total_sz += r->ar.end - r->ar.start;
+		}
+	}
+	sort(region_pointers, nr_regions, sizeof(*region_pointers),
+			damon_stat_cmp_regions, NULL);
+	*sorted_ptr = region_pointers;
+	*nr_regions_ptr = nr_regions;
+	*total_sz_ptr = total_sz;
+	return 0;
+}
+
+static void damon_stat_set_idletime_percentiles(struct damon_ctx *c)
+{
+	struct damon_region **sorted_regions, *region;
+	int nr_regions;
+	unsigned long total_sz, accounted_bytes = 0;
+	int err, i, next_percentile = 0;
+
+	err = damon_stat_sort_regions(c, &sorted_regions, &nr_regions,
+			&total_sz);
+	if (err)
+		return;
+	for (i = 0; i < nr_regions; i++) {
+		region = sorted_regions[i];
+		accounted_bytes += region->ar.end - region->ar.start;
+		while (next_percentile <= accounted_bytes * 100 / total_sz)
+			percentiles_ms[next_percentile++] =
+				damon_stat_idletime(region) *
+				c->attrs.aggr_interval / USEC_PER_MSEC;
+	}
+	kfree(sorted_regions);
+}
+
 static int damon_stat_after_aggregation(struct damon_ctx *c)
 {
 	struct damon_target *t;
@@ -45,6 +115,7 @@ static int damon_stat_after_aggregation(struct damon_ctx *c)
 	}
 	estimated_memory_bandwidth = access_bytes * USEC_PER_MSEC *
 		MSEC_PER_SEC / c->attrs.aggr_interval;
+	damon_stat_set_idletime_percentiles(c);
 	return 0;
 }
 
-- 
2.39.5

