From 7eaabfafd4947bc9df303edadaf07efd988208e2 Mon Sep 17 00:00:00 2001
From: SeongJae Park <sj@kernel.org>
Date: Sat, 11 May 2024 12:51:31 -0700
Subject: [PATCH] simplify

Signed-off-by: SeongJae Park <sj@kernel.org>
---
 mm/damon/paddr.c | 176 ++++++++++++++++++++++++-----------------------
 1 file changed, 91 insertions(+), 85 deletions(-)

diff --git a/mm/damon/paddr.c b/mm/damon/paddr.c
index f4de97a9cdb9..a468792d9162 100644
--- a/mm/damon/paddr.c
+++ b/mm/damon/paddr.c
@@ -243,10 +243,90 @@ static bool damos_pa_filter_out(struct damos *scheme, struct folio *folio)
 	return false;
 }
 
-enum migration_mode {
-	DAMOS_MIG_PAGEOUT,
-	MIG_MIGRATE_COLD,
-};
+static unsigned long damon_pa_pageout(struct damon_region *r, struct damos *s)
+{
+	unsigned long addr, applied;
+	LIST_HEAD(folio_list);
+	bool install_young_filter = true;
+	struct damos_filter *filter;
+
+	/* check access in page level again by default */
+	damos_for_each_filter(filter, s) {
+		if (filter->type == DAMOS_FILTER_TYPE_YOUNG) {
+			install_young_filter = false;
+			break;
+		}
+	}
+	if (install_young_filter) {
+		filter = damos_new_filter(DAMOS_FILTER_TYPE_YOUNG, true);
+		if (!filter)
+			return 0;
+		damos_add_filter(s, filter);
+	}
+
+	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
+		struct folio *folio = damon_get_folio(PHYS_PFN(addr));
+
+		if (!folio)
+			continue;
+
+		if (damos_pa_filter_out(s, folio))
+			goto put_folio;
+
+		folio_clear_referenced(folio);
+		folio_test_clear_young(folio);
+		if (!folio_isolate_lru(folio))
+			goto put_folio;
+		if (folio_test_unevictable(folio))
+			folio_putback_lru(folio);
+		else
+			list_add(&folio->lru, &folio_list);
+put_folio:
+		folio_put(folio);
+	}
+	if (install_young_filter)
+		damos_destroy_filter(filter);
+	applied = reclaim_pages(&folio_list);
+	cond_resched();
+	return applied * PAGE_SIZE;
+}
+
+static inline unsigned long damon_pa_mark_accessed_or_deactivate(
+		struct damon_region *r, struct damos *s, bool mark_accessed)
+{
+	unsigned long addr, applied = 0;
+
+	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
+		struct folio *folio = damon_get_folio(PHYS_PFN(addr));
+
+		if (!folio)
+			continue;
+
+		if (damos_pa_filter_out(s, folio))
+			goto put_folio;
+
+		if (mark_accessed)
+			folio_mark_accessed(folio);
+		else
+			folio_deactivate(folio);
+		applied += folio_nr_pages(folio);
+put_folio:
+		folio_put(folio);
+	}
+	return applied * PAGE_SIZE;
+}
+
+static unsigned long damon_pa_mark_accessed(struct damon_region *r,
+	struct damos *s)
+{
+	return damon_pa_mark_accessed_or_deactivate(r, s, true);
+}
+
+static unsigned long damon_pa_deactivate_pages(struct damon_region *r,
+	struct damos *s)
+{
+	return damon_pa_mark_accessed_or_deactivate(r, s, false);
+}
 
 static unsigned int __damon_pa_migrate_folio_list(
 		struct list_head *migrate_folios, struct pglist_data *pgdat,
@@ -260,8 +340,8 @@ static unsigned int __damon_pa_migrate_folio_list(
 		 * When this happens, 'page' will likely just be discarded
 		 * instead of migrated.
 		 */
-		.gfp_mask = (GFP_HIGHUSER_MOVABLE & ~__GFP_RECLAIM) | __GFP_NOWARN |
-			__GFP_NOMEMALLOC | GFP_NOWAIT,
+		.gfp_mask = (GFP_HIGHUSER_MOVABLE & ~__GFP_RECLAIM) |
+			__GFP_NOWARN | __GFP_NOMEMALLOC | GFP_NOWAIT,
 		.nid = target_nid,
 		.nmask = &allowed_mask
 	};
@@ -369,27 +449,10 @@ static unsigned long damon_pa_migrate_pages(struct list_head *folio_list,
 	return nr_migrated;
 }
 
-static unsigned long damon_pa_migrate(struct damon_region *r, struct damos *s,
-				      enum migration_mode mode)
+static unsigned long damon_pa_migrate(struct damon_region *r, struct damos *s)
 {
 	unsigned long addr, applied;
 	LIST_HEAD(folio_list);
-	bool install_young_filter = true;
-	struct damos_filter *filter;
-
-	/* check access in page level again by default */
-	damos_for_each_filter(filter, s) {
-		if (filter->type == DAMOS_FILTER_TYPE_YOUNG) {
-			install_young_filter = false;
-			break;
-		}
-	}
-	if (install_young_filter) {
-		filter = damos_new_filter(DAMOS_FILTER_TYPE_YOUNG, true);
-		if (!filter)
-			return 0;
-		damos_add_filter(s, filter);
-	}
 
 	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
 		struct folio *folio = damon_get_folio(PHYS_PFN(addr));
@@ -400,74 +463,17 @@ static unsigned long damon_pa_migrate(struct damon_region *r, struct damos *s,
 		if (damos_pa_filter_out(s, folio))
 			goto put_folio;
 
-		folio_clear_referenced(folio);
-		folio_test_clear_young(folio);
 		if (!folio_isolate_lru(folio))
 			goto put_folio;
-		/*
-		 * Since unevictable folios can be migrated, unevictable test
-		 * is needed only for pageout.
-		 */
-		if (mode == DAMOS_MIG_PAGEOUT && folio_test_unevictable(folio))
-			folio_putback_lru(folio);
-		else
-			list_add(&folio->lru, &folio_list);
+		list_add(&folio->lru, &folio_list);
 put_folio:
 		folio_put(folio);
 	}
-	if (install_young_filter)
-		damos_destroy_filter(filter);
-	switch (mode) {
-	case DAMOS_MIG_PAGEOUT:
-		applied = reclaim_pages(&folio_list);
-		break;
-	case MIG_MIGRATE_COLD:
-		applied = damon_pa_migrate_pages(&folio_list, s->target_nid);
-		break;
-	default:
-		/* Unexpected migration mode. */
-		return 0;
-	}
+	applied = damon_pa_migrate_pages(&folio_list, s->target_nid);
 	cond_resched();
 	return applied * PAGE_SIZE;
 }
 
-static inline unsigned long damon_pa_mark_accessed_or_deactivate(
-		struct damon_region *r, struct damos *s, bool mark_accessed)
-{
-	unsigned long addr, applied = 0;
-
-	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
-		struct folio *folio = damon_get_folio(PHYS_PFN(addr));
-
-		if (!folio)
-			continue;
-
-		if (damos_pa_filter_out(s, folio))
-			goto put_folio;
-
-		if (mark_accessed)
-			folio_mark_accessed(folio);
-		else
-			folio_deactivate(folio);
-		applied += folio_nr_pages(folio);
-put_folio:
-		folio_put(folio);
-	}
-	return applied * PAGE_SIZE;
-}
-
-static unsigned long damon_pa_mark_accessed(struct damon_region *r,
-	struct damos *s)
-{
-	return damon_pa_mark_accessed_or_deactivate(r, s, true);
-}
-
-static unsigned long damon_pa_deactivate_pages(struct damon_region *r,
-	struct damos *s)
-{
-	return damon_pa_mark_accessed_or_deactivate(r, s, false);
-}
 
 static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
 		struct damon_target *t, struct damon_region *r,
@@ -475,13 +481,13 @@ static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
 {
 	switch (scheme->action) {
 	case DAMOS_PAGEOUT:
-		return damon_pa_migrate(r, scheme, DAMOS_MIG_PAGEOUT);
+		return damon_pa_pageout(r, scheme);
 	case DAMOS_LRU_PRIO:
 		return damon_pa_mark_accessed(r, scheme);
 	case DAMOS_LRU_DEPRIO:
 		return damon_pa_deactivate_pages(r, scheme);
 	case DAMOS_MIGRATE_COLD:
-		return damon_pa_migrate(r, scheme, MIG_MIGRATE_COLD);
+		return damon_pa_migrate(r, scheme);
 	case DAMOS_STAT:
 		break;
 	default:
-- 
2.39.2

